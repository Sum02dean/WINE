{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import typing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "from torch.distributions import Normal\n",
    "from torch import distributions as dis\n",
    "\n",
    "class ParameterDistribution(torch.nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class that models a distribution over model parameters,\n",
    "    usable for Bayes by backprop.\n",
    "    You can implement this class using any distribution you want\n",
    "    and try out different priors and variational posteriors.\n",
    "    All torch.nn.Parameter that you add in the __init__ method of this class\n",
    "    will automatically be registered and know to PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of the given values\n",
    "        :param values: Values to calculate the log-likelihood on\n",
    "        :return: Log-likelihood\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from this distribution.\n",
    "        Note that you only need to implement this method for variational posteriors, not priors.\n",
    "\n",
    "        :return: Sample from this distribution. The sample shape depends on your semantics.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # DO NOT USE THIS METHOD\n",
    "        # We only implement it since torch.nn.Module requires a forward method\n",
    "        warnings.warn('ParameterDistribution should not be called! Use its explicit methods!')\n",
    "        return self.log_likelihood(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniveriateGaussianPrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univeriate Guassian distribution\"\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        super(UniveriateGaussianPrior, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma # sigma is the standard deviation here, not the variance!\n",
    "    \n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist = Normal(loc=self.mu, scale=self.sigma)\n",
    "        log_likelihood = dist.log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self):\n",
    "       return Normal(loc=self.mu, scale=self.sigma).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sig = (F.softplus(rho)*0.05 + 1e-5).detach()\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        log_likelihood = Normal(self.mu, self.sig).log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        epsilon = torch.distributions.Normal(0,1).sample(self.rho.size())\n",
    "        return self.mu + self.sig*epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixturePrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Mixture of two Gaussian distributions as described in Bludell et al., 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_0: torch.Tensor, sigma_0: torch.Tensor, mu_1: torch.Tensor, sigma_1: torch.Tensor, pi: torch.Tensor):\n",
    "        super(GaussianMixturePrior, self).__init__()  # always make sure to include the super-class init call!\n",
    "        self.mu_0 = mu_0 # mean of distribution 0\n",
    "        self.sigma_0 = sigma_0 # std of distrinution 0\n",
    "        self.mu_1 = mu_1 # mean of distribution 1\n",
    "        self.sigma_1 = sigma_1 # std of distribution 1\n",
    "        self.pi = pi # Probabilistic weight\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist_0 = Normal(loc=self.mu_0, scale=self.sigma_0)\n",
    "        dist_1 = Normal(loc=self.mu_1, scale=self.sigma_1)\n",
    "        ll_0 = dist_0.log_prob(values)\n",
    "        ll_1 = dist_1.log_prob(values)\n",
    "        return torch.log(self.pi * torch.exp(ll_0) + (1 - self.pi) * torch.exp(ll_1)).sum()\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        if np.random.rand() < self.pi:\n",
    "            return Normal(loc=self.mu_0, scale=self.sigma_0).sample()\n",
    "        else:\n",
    "            return Normal(loc=self.mu_1, scale=self.sigma_1).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # Set hyper priors\n",
    "        mu_0_prior =  torch.tensor(0.0)\n",
    "        sigma_0_prior = torch.tensor(0.368)\n",
    "        mu_1_prior = torch.tensor(0.0)\n",
    "        sigma_1_prior = torch.tensor(0.00091)\n",
    "        pi_prior = torch.tensor(0.5)\n",
    "\n",
    "        # Define prior distribution\n",
    "        self.prior = GaussianMixturePrior(mu_0 = mu_0_prior,\n",
    "                                          sigma_0 = sigma_0_prior,\n",
    "                                          mu_1 = mu_1_prior,\n",
    "                                          sigma_1 = sigma_1_prior,\n",
    "                                          pi = pi_prior\n",
    "        )\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.prior.parameters()), 'Prior SHOULD NOT have parameters'\n",
    "\n",
    "        # Set intitial hyper poteriors\n",
    "        std_mu_init = torch.tensor(0.1) # Mean\n",
    "        std_rho_init = torch.tensor(1.) # Parameterisation of Std\n",
    "        \n",
    "        # Initialize weights by sampling from normal distributions\n",
    "        w_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features, in_features))\n",
    "        w_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features, in_features))\n",
    "\n",
    "        # Create parameter distributions\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(w_mu_init),\n",
    "            rho = torch.nn.Parameter(w_rho_init)\n",
    "        )\n",
    "\n",
    "        # Error check\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # TODO: As for the weights, create the bias variational posterior instance here.\n",
    "            #  Make sure to follow the same rules as for the weight variational posterior.\n",
    "\n",
    "            b_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features,))\n",
    "            b_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features,))\n",
    "\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(b_mu_init),\n",
    "            rho = torch.nn.Parameter(b_rho_init)\n",
    "        )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        \"\"\"\n",
    "        # Sample the weights\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "\n",
    "        # Generate the log-liklihood of the prior and log-posterior\n",
    "        log_prior = self.prior.log_likelihood(weights)\n",
    "        log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Sample the bias posterios and get prior log-likelihood\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "            log_prior += self.prior.log_likelihood(bias)\n",
    "            # Add on the terms to the variatinoal posterior\n",
    "            log_variational_posterior += self.bias_var_posterior.log_likelihood(bias)\n",
    "        else:\n",
    "            bias = None\n",
    "        # Call low level linear alg operation\n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a Bayesian feedforward neural network using BayesianLayer objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a BNN.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a (Bayesian) hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform one forward pass through the BNN using a single set of weights\n",
    "        sampled from the variational posterior.\n",
    "\n",
    "        :param x: Input features, float tensor of shape (batch_size, in_features)\n",
    "        :return: 3-tuple containing\n",
    "            i) output features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "\n",
    "        log_prior = torch.tensor(0.0)\n",
    "        log_variational_posterior = torch.tensor(0.0)\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            x, log_prior_layer, log_variational_posterior_layer = current_layer(x)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "\n",
    "            log_prior += log_prior_layer\n",
    "            log_variational_posterior += log_variational_posterior_layer\n",
    "\n",
    "        \"\"\"\n",
    "        Summing up the log_variational_posterior across all layers is necessary because the \n",
    "        variational inference process aims to minimize the Kullback-Leibler (KL) divergence between the variational \n",
    "        posterior and the true posterior. Summing the logarithms of the variational posterior probabilities from all\n",
    "          layers allows the network to capture the overall uncertainty in the model's parameters. \n",
    "        This is an essential aspect of training Bayesian Neural Networks using variational inference.\n",
    "        \"\"\"\n",
    "\n",
    "        return x, log_prior, log_variational_posterior\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given features by sampling from this BNN.\n",
    "\n",
    "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
    "        :param num_mc_samples: Number of MC samples to take for prediction\n",
    "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
    "            such that the last dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
    "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
    "\n",
    "        assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0))\n",
    "        return estimated_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "    BNN using Bayes by backprop\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Hyperparameters and general parameters\n",
    "        self.num_epochs = 20  # number of training epochs\n",
    "        self.batch_size = 128  # training batch size\n",
    "        learning_rate = 1e-3  # training learning rates\n",
    "        hidden_layers = (400, 400)  # for each entry, creates a hidden layer with the corresponding number of units\n",
    "        self.print_interval = 100  # number of batches until updated metrics are displayed during training\n",
    "        self.n_mcs = 1 # Number of monte-carlo samples\n",
    "\n",
    "        # BayesNet\n",
    "        print('Using a BayesNet model')\n",
    "        self.network = BayesNet(in_features=28 * 28, hidden_features=hidden_layers, out_features=10)\n",
    "\n",
    "        # Optimizer for training\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
