{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import typing\n",
    "import json\n",
    "from torch.nn.functional import one_hot\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "from torch.distributions import Normal\n",
    "from torch import distributions as dis\n",
    "\n",
    "# UNDERSTOOD\n",
    "class ParameterDistribution(torch.nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class that models a distribution over model parameters,\n",
    "    usable for Bayes by backprop.\n",
    "    You can implement this class using any distribution you want\n",
    "    and try out different priors and variational posteriors.\n",
    "    All torch.nn.Parameter that you add in the __init__ method of this class\n",
    "    will automatically be registered and know to PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of the given values\n",
    "        :param values: Values to calculate the log-likelihood on\n",
    "        :return: Log-likelihood\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from this distribution.\n",
    "        Note that you only need to implement this method for variational posteriors, not priors.\n",
    "\n",
    "        :return: Sample from this distribution. The sample shape depends on your semantics.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # DO NOT USE THIS METHOD\n",
    "        # We only implement it since torch.nn.Module requires a forward method\n",
    "        warnings.warn('ParameterDistribution should not be called! Use its explicit methods!')\n",
    "        return self.log_likelihood(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDERSTOOD\n",
    "class UniveriateGaussianPrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univeriate Guassian distribution\"\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        super(UniveriateGaussianPrior, self).__init__()\n",
    "        self.mu = mu # Distribution mean\n",
    "        self.sigma = sigma # sigma is the distribution standard deviation here, not the variance!\n",
    "    \n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute the log-likelihood of the given values\n",
    "        dist = Normal(loc=self.mu, scale=self.sigma)\n",
    "        log_likelihood = dist.log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self):\n",
    "        # Sample from the pior distribution\n",
    "       return Normal(loc=self.mu, scale=self.sigma).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom DataSet class for Pytorch models\"\"\"\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return np.shape(self.features)[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        x = self.features[index]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "        else:\n",
    "            y = x\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "# UNDERSTOOD\n",
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sig = (F.softplus(rho)*0.05 + 1e-5).detach()\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist = Normal(loc=self.mu, scale=self.sig)\n",
    "        log_likelihood = dist.log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        epsilon = torch.distributions.Normal(0,1).sample(self.rho.size())\n",
    "        return self.mu + self.sig*epsilon\n",
    "\n",
    "# UNDERSTOOD\n",
    "class GaussianMixturePrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Mixture of two Gaussian distributions as described in Bludell et al., 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_0: torch.Tensor, sigma_0: torch.Tensor, mu_1: torch.Tensor, sigma_1: torch.Tensor, pi: torch.Tensor):\n",
    "        super(GaussianMixturePrior, self).__init__()  # always make sure to include the super-class init call!\n",
    "        self.mu_0 = mu_0 # mean of distribution 0\n",
    "        self.sigma_0 = sigma_0 # std of distrinution 0\n",
    "        self.mu_1 = mu_1 # mean of distribution 1\n",
    "        self.sigma_1 = sigma_1 # std of distribution 1\n",
    "        self.pi = pi # Probabilistic weight\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist_0 = Normal(loc=self.mu_0, scale=self.sigma_0)\n",
    "        dist_1 = Normal(loc=self.mu_1, scale=self.sigma_1)\n",
    "        ll_0 = dist_0.log_prob(values)\n",
    "        ll_1 = dist_1.log_prob(values)\n",
    "        # https://www.youtube.com/watch?v=qMTuMa86NzU\n",
    "        return torch.log(self.pi * torch.exp(ll_0) + (1 - self.pi) * torch.exp(ll_1)).sum() # is pi the size  or is it propbability it is both!\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # Creates a mixture of the two distributions depending on the size parameter pi\n",
    "        if np.random.rand() < self.pi:\n",
    "            return Normal(loc=self.mu_0, scale=self.sigma_0).sample()\n",
    "        else:\n",
    "            return Normal(loc=self.mu_1, scale=self.sigma_1).sample()\n",
    "        \n",
    "#UNDERSTOOD\n",
    "class BayesMultiLoss():\n",
    "    \n",
    "    \"\"\" Computes the KLD + NLL multi-objective loss. KLD is computed as mean of n-bathches. \n",
    "        The final loss is given as a mean over n-monte-carlo samples of the outputs returned by the forward pass of\n",
    "        BayesNet.\"\"\"\n",
    "    def __init__(self, net_outputs, targets, log_posterior, log_prior,\n",
    "                 batch_size, num_batches, method='exact'):\n",
    "        \n",
    "        # Define fields\n",
    "        self.net_outputs = net_outputs # Forward pass outputs\n",
    "        self.targets = targets         # y_batch targets\n",
    "        self.log_posterior=log_posterior # Log post\n",
    "        self.log_prior=log_prior     # Log prior\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.num_batches = num_batches # Number of batches\n",
    "        self.method = method # KLD method: exact or approx\n",
    "    \n",
    "    def __compute_kld_loss(self):\n",
    "        \"\"\" Computes the kld loss\"\"\"\n",
    "\n",
    "        if self.method == 'exact':\n",
    "            kld = self.log_posterior - self.log_prior\n",
    "            kld_scaled = kld / self.num_batches \n",
    "            return kld_scaled\n",
    "    \n",
    "        elif self.method =='approx':\n",
    "            # Note the reciprocal in log_ratio as described in link below for reverse approx KLD:\n",
    "            # https://towardsdatascience.com/approximating-kl-divergence-4151c8c85ddd\n",
    "            log_ratio = self.log_prior - self.log_posterior\n",
    "            kld = (log_ratio.exp() -1) - log_ratio \n",
    "            kld_scaled = kld / self.num_batches\n",
    "        return kld_scaled\n",
    "            \n",
    "    def __compute_nll_loss(self):\n",
    "        \"\"\" Computes the NLL loss\"\"\"\n",
    "        loss = F.nll_loss(F.log_softmax(self.net_outputs, dim=1), self.targets, reduction='sum') \n",
    "        return loss\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        \"\"\" Computes the combined loss: KLD + NLL\"\"\"\n",
    "        kld = self.__compute_kld_loss()\n",
    "        nll = self.__compute_nll_loss()\n",
    "        multi_loss = kld + nll\n",
    "        return multi_loss\n",
    "    \n",
    "# UNDERSTOOD - What we are optimising are th mu's and the sigam's. Then we sample the layer weights, and compute use them to compute the layer \n",
    "# ... outputs using a simple F.Linear layer. This is performed for all layers in the network, we get an error signal, which then backpropogates, NOT on the \n",
    "# ... weights of the layer, but on the mu's and sigam's themselves which are then sampled from to generate the next set of weights.\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # Set hyper priors: doubled because using guassian mixture\n",
    "        mu_0_prior =  torch.tensor(0.0)\n",
    "        sigma_0_prior = torch.tensor(0.368)\n",
    "        mu_1_prior = torch.tensor(0.0)\n",
    "        sigma_1_prior = torch.tensor(0.00091)\n",
    "        pi_prior = torch.tensor(0.5)\n",
    "\n",
    "        # Define prior distribution\n",
    "        self.prior = GaussianMixturePrior(mu_0 = mu_0_prior,\n",
    "                                          sigma_0 = sigma_0_prior,\n",
    "                                          mu_1 = mu_1_prior,\n",
    "                                          sigma_1 = sigma_1_prior,\n",
    "                                          pi = pi_prior\n",
    "        )\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.prior.parameters()), 'Prior SHOULD NOT have parameters'\n",
    "\n",
    "        # Set intitial hyper poteriors\n",
    "        std_mu_init = torch.tensor(0.1) # Posterior distribution initial mean (multivariate diagonal guassian)\n",
    "        std_rho_init = torch.tensor(1.) # Posterio distribution Parameterisation of Std (multivariate diagonal guassian)\n",
    "        \n",
    "        # Initialize weights by sampling from normal distributions\n",
    "        # ... ( in_features = number of neurons in layer-1, out_features = number of connections going into layer)\n",
    "        w_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features, in_features))\n",
    "        w_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features, in_features))\n",
    "\n",
    "        # Convert sampled weights into torch parameters for optimisation\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(w_mu_init),\n",
    "            rho = torch.nn.Parameter(w_rho_init)\n",
    "        )\n",
    "\n",
    "        # Error check\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Initialize bias with zero mean and with parameterised std - samples n times where n=out_features\n",
    "            # ... each layer has out_features weights\n",
    "            b_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features,))\n",
    "            b_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features,))\n",
    "\n",
    "            # Use the same posterior family distribution and make them torch parameters for optimisation\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(b_mu_init),\n",
    "            rho = torch.nn.Parameter(b_rho_init)\n",
    "        )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        \"\"\"\n",
    "        # Sample the weights (1st round is from initialised posterior)\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "\n",
    "        # Generate the log-liklihood of the prior and log-posterior\n",
    "        # Here we are summing the log-likelihood of the weights to see how well the weights can explain the data\n",
    "        # ... the greater the log-liklihood the better the weights can generate the observed data\n",
    "        log_prior = self.prior.log_likelihood(weights)\n",
    "        log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "\n",
    "        # As in standard machine learning, we simply add on the bias term to each output in the next adjacent layer\n",
    "        if self.use_bias:\n",
    "            # Sample the bias posterios and get prior log-likelihood\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "\n",
    "            log_prior += self.prior.log_likelihood(bias)\n",
    "            # Add on the terms to the variatinoal posterior\n",
    "            log_variational_posterior += self.bias_var_posterior.log_likelihood(bias)\n",
    "        else:\n",
    "            bias = None\n",
    "        # Call low level linear alg operation, this is single layer architecture\n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior\n",
    "    \n",
    "# UNDERSTOOD - What we are optimising are th mu's and the sigam's. Then we sample the layer weights, and compute use them to compute the layer \n",
    "# ... outputs using a simple F.Linear layer. This is performed for all layers in the network, we get an error signal, which then backpropogates, NOT on the \n",
    "# ... weights of the layer, but on the mu's and sigam's themselves which are then sampled from to generate the next set of weights.\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # Set hyper priors: doubled because using guassian mixture\n",
    "        mu_0_prior =  torch.tensor(0.0)\n",
    "        sigma_0_prior = torch.tensor(0.368)\n",
    "        mu_1_prior = torch.tensor(0.0)\n",
    "        sigma_1_prior = torch.tensor(0.00091)\n",
    "        pi_prior = torch.tensor(0.5)\n",
    "\n",
    "        # Define prior distribution\n",
    "        self.prior = GaussianMixturePrior(mu_0 = mu_0_prior,\n",
    "                                          sigma_0 = sigma_0_prior,\n",
    "                                          mu_1 = mu_1_prior,\n",
    "                                          sigma_1 = sigma_1_prior,\n",
    "                                          pi = pi_prior\n",
    "        )\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.prior.parameters()), 'Prior SHOULD NOT have parameters'\n",
    "\n",
    "        # Set intitial hyper poteriors\n",
    "        std_mu_init = torch.tensor(0.1) # Posterior distribution initial mean (multivariate diagonal guassian)\n",
    "        std_rho_init = torch.tensor(1.) # Posterio distribution Parameterisation of Std (multivariate diagonal guassian)\n",
    "        \n",
    "        # Initialize weights by sampling from normal distributions\n",
    "        # ... ( in_features = number of neurons in layer-1, out_features = number of connections going into layer)\n",
    "        w_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features, in_features))\n",
    "        w_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features, in_features))\n",
    "\n",
    "        # Convert sampled weights into torch parameters for optimisation\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(w_mu_init),\n",
    "            rho = torch.nn.Parameter(w_rho_init)\n",
    "        )\n",
    "\n",
    "        # Error check\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Initialize bias with zero mean and with parameterised std - samples n times where n=out_features\n",
    "            # ... each layer has out_features weights\n",
    "            b_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features,))\n",
    "            b_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features,))\n",
    "\n",
    "            # Use the same posterior family distribution and make them torch parameters for optimisation\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(b_mu_init),\n",
    "            rho = torch.nn.Parameter(b_rho_init)\n",
    "        )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        \"\"\"\n",
    "        # Sample the weights (1st round is from initialised posterior)\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "\n",
    "        # Generate the log-liklihood of the prior and log-posterior\n",
    "        # Here we are summing the log-likelihood of the weights to see how well the weights can explain the data\n",
    "        # ... the greater the log-liklihood the better the weights can generate the observed data\n",
    "        log_prior = self.prior.log_likelihood(weights)\n",
    "        log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "\n",
    "        # As in standard machine learning, we simply add on the bias term to each output in the next adjacent layer\n",
    "        if self.use_bias:\n",
    "            # Sample the bias posterios and get prior log-likelihood\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "\n",
    "            log_prior += self.prior.log_likelihood(bias)\n",
    "            # Add on the terms to the variatinoal posterior\n",
    "            log_variational_posterior += self.bias_var_posterior.log_likelihood(bias)\n",
    "        else:\n",
    "            bias = None\n",
    "        # Call low level linear alg operation, this is single layer architecture\n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior\n",
    "    \n",
    "# UNDERSTOOD - Simply returns the log-likelihood summed over all layer weights and the predicted ouputs\n",
    "class BayesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a Bayesian feedforward neural network using BayesianLayer objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a BNN.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a (Bayesian) hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # Dynamically build the number of layers and their sizes\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform one forward pass through the BNN using a single set of weights\n",
    "        sampled from the variational posterior.\n",
    "\n",
    "        :param x: Input features, float tensor of shape (batch_size, in_features)\n",
    "        :return: 3-tuple containing\n",
    "            i) output features using stochastic weights from the variational posterior,\n",
    "            ii) sample of the log-prior probability, and\n",
    "            iii) sample of the log-variational-posterior probability\n",
    "        \"\"\"\n",
    "        # Initialize the \"summed\" log-prior likelihood and log-variational-posterior likelihood\n",
    "        log_prior = torch.tensor(0.0)\n",
    "        log_variational_posterior = torch.tensor(0.0)\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            x, log_prior_layer, log_variational_posterior_layer = current_layer(x)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "\n",
    "            log_prior += log_prior_layer\n",
    "            log_variational_posterior += log_variational_posterior_layer\n",
    "\n",
    "        \"\"\"\n",
    "        Summing up the log_variational_posterior across all layers is necessary because the \n",
    "        variational inference process aims to minimize the Kullback-Leibler (KL) divergence between the variational \n",
    "        posterior and the true posterior. Summing the logarithms of the variational posterior probabilities from all\n",
    "          layers allows the network to capture the overall uncertainty in the model's parameters. \n",
    "        This is an essential aspect of training Bayesian Neural Networks using variational inference.\n",
    "        \"\"\"\n",
    "\n",
    "        return x, log_prior, log_variational_posterior\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given features by sampling from this BNN.\n",
    "\n",
    "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
    "        :param num_mc_samples: Number of MC samples to take for prediction\n",
    "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
    "            such that the last dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "\n",
    "        # There are two types of monto-carlo samples in this scriot - the one below is used to generate a predictive distribution\n",
    "        # and therefore to model the uncertainty in the output predictions.\n",
    "        # ... While the model monte-carlo samples is used to approximate the posterior distribution of the weight distribution parameters\n",
    "        # ... as the exact posterior requires integration over sets of guassians, and this is extremely difficult and/or intractable.\n",
    "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
    "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
    "\n",
    "        # assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0)) # Make sure the probabilities add up to 1\n",
    "        return estimated_probability, probability_samples\n",
    "    \n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    BNN using Bayes by backprop\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Hyperparameters and general parameters\n",
    "        self.num_epochs = 1000  # number of training epochs\n",
    "        self.batch_size = 128  # training batch size\n",
    "        learning_rate = 1e-2  # training learning rates\n",
    "        hidden_layers = (100, 100)  # for each entry, creates a hidden layer with the corresponding number of units\n",
    "        self.print_interval = 100  # number of batches until updated metrics are displayed during training\n",
    "        self.n_mcs = 5 # Number of monte-carlo samples\n",
    "\n",
    "        # BayesNet\n",
    "        print('Using a BayesNet model')\n",
    "        self.network = BayesNet(in_features=13, hidden_features=hidden_layers, out_features=2)\n",
    "\n",
    "        # Optimizer for training\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def fit(self, dataset: torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Train your neural network.\n",
    "        If the network is a DenseNet, this performs normal stochastic gradient descent training.\n",
    "        If the network is a BayesNet, this should perform Bayes by backprop.\n",
    "\n",
    "        :param dataset: Dataset you should use for training\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "        # Place network into training mode\n",
    "        self.network.train()\n",
    "\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "        for _ in progress_bar:\n",
    "            num_batches = len(train_loader)\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                # batch_x are of shape (batch_size, 13), batch_y are of shape (batch_size,)\n",
    "                # Convert the data-types\n",
    "                batch_x = torch.FloatTensor(batch_x)\n",
    "                self.network.zero_grad()\n",
    "            \n",
    "                # BayesNet training step via Bayes by backprop\n",
    "                assert isinstance(self.network, BayesNet)\n",
    "\n",
    "                # TODO: Implement Bayes by backprop training here\n",
    "                # Deans code goes here.\n",
    "                \n",
    "                #1. Run forwrd pass on BayesNet\n",
    "                #2. Extract the [outputs, log-prior, log-posterior]\n",
    "                #3. Instantiate BayesMultiLoss class\n",
    "                #4. Calculate combinatorial loss\n",
    "                #5. Generate MCS mean loss.  \n",
    "                #5. Back-propogate the gradients  - issue with retaining graphs atm.    \n",
    "                \n",
    "                loss = torch.tensor([0.0])\n",
    "                for _ in range(self.n_mcs):\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    outputs, log_prior, log_posterior = current_logits\n",
    "                    \n",
    "                    # Compute the losses \n",
    "                    batch_size = train_loader.batch_size\n",
    "                    BML = BayesMultiLoss(net_outputs=outputs, targets=batch_y, \n",
    "                                            log_posterior=log_posterior, log_prior=log_prior,\n",
    "                                            batch_size=batch_size, num_batches=num_batches, method='approx')\n",
    "                    \n",
    "                    loss += BML.compute_loss()\n",
    "                    \n",
    "                # Backpropagate to get the gradients\n",
    "                loss = loss/self.n_mcs\n",
    "                loss.backward()\n",
    "                    \n",
    "                \n",
    "                # Step the gradients\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Update progress bar with accuracy occasionally\n",
    "                if batch_idx % self.print_interval == 0:    \n",
    "                    current_logits, _, _ = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
    "                \n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Predict the class probabilities using your trained model.\n",
    "            This method should return an (num_samples, 10) NumPy float array\n",
    "            such that the second dimension sums up to 1 for each row.\n",
    "\n",
    "            :param data_loader: Data loader yielding the samples to predict on\n",
    "            :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "            \"\"\"\n",
    "\n",
    "            self.network.eval()\n",
    "\n",
    "            probability_batches = []\n",
    "            predictive_probability_dist_batches = []\n",
    "            for batch_x, _ in data_loader:\n",
    "                batch_x = torch.FloatTensor(batch_x)\n",
    "                current_probabilities, predictive_distribution = self.network.predict_probabilities(batch_x)\n",
    "                current_probabilities  = current_probabilities.detach().numpy()\n",
    "                predictive_distribution = predictive_distribution.detach().numpy()\n",
    "                print(current_probabilities.shape)\n",
    "                probability_batches.append(current_probabilities)\n",
    "                predictive_probability_dist_batches.append(predictive_distribution)\n",
    "\n",
    "            output = np.concatenate(probability_batches, axis=0)\n",
    "            output_pred_dist = predictive_probability_dist_batches\n",
    "            assert isinstance(output, np.ndarray)\n",
    "            # assert output.ndim == 2 and output.shape[1] == 10\n",
    "            assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "            return output, output_pred_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a BayesNet model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 892/1000 [05:10<00:38,  2.81it/s, acc=0.797, loss=1.11e+3]"
     ]
    }
   ],
   "source": [
    "# Read in the configs file\n",
    "FILE_NAME = \"/Users/sum02dean/projects/wine_challenge/WINE/configs/config_file.json\"\n",
    "\n",
    "# Read in the parameters from configs file\n",
    "with open(FILE_NAME, encoding='utf-8') as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "# Extract parameters\n",
    "mlflow_params = configs.get(\"mlflow_params\")\n",
    "model_params = configs.get(\"model_params\")\n",
    "data_params = configs.get(\"data_params\")\n",
    "\n",
    "# Features\n",
    "train_x_raw = pd.read_csv(data_params['x_train_path'])\n",
    "test_x_raw  = pd.read_csv(data_params['x_test_path'])\n",
    "\n",
    "# Labels\n",
    "train_y_raw  = pd.read_csv(data_params['y_train_path'])\n",
    "test_y_raw  = pd.read_csv(data_params['y_test_path'])\n",
    "\n",
    "y_train = torch.LongTensor(train_y_raw.to_numpy()).squeeze(-1)\n",
    "x_train = torch.FloatTensor(train_x_raw.to_numpy())\n",
    "\n",
    "y_test = torch.LongTensor(test_y_raw.to_numpy()).squeeze(-1)\n",
    "x_test = torch.FloatTensor(test_x_raw.to_numpy())\n",
    "\n",
    "# Initialize the train dataloaders\n",
    "train_dataset = MyDataset(features=x_train, labels=y_train)\n",
    "test_dataset = MyDataset(features=x_test, labels=y_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)\n",
    "\n",
    "# Instantiate run and predict\n",
    "model = Model()\n",
    "model.fit(train_dataset)\n",
    "predictions, predictive_distributions = model.predict(test_loader)\n",
    "\n",
    "# Get accuracy report\n",
    "predicted_classes = [np.argmax(x) for x in predictions]\n",
    "target_labels  = list(y_test.numpy())\n",
    "df_pred = pd.DataFrame(list(zip(target_labels, predicted_classes)), columns=['actual', 'predicted'])\n",
    "df_pred['correct'] = df_pred['actual'] == df_pred['predicted']\n",
    "\n",
    "from collections import Counter\n",
    "# Get ratio of true to false in correct column \n",
    "accuracy = df_pred['correct'].sum() / len(df_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is without comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom DataSet class for Pytorch models\"\"\"\n",
    "    def __init__(self, features, labels=None):\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return np.shape(self.features)[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        x = self.features[index]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            y = self.labels[index]\n",
    "        else:\n",
    "            y = x\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sig = (F.softplus(rho)*0.05 + 1e-5).detach()\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist = Normal(loc=self.mu, scale=self.sig)\n",
    "        log_likelihood = dist.log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        epsilon = torch.distributions.Normal(0,1).sample(self.rho.size())\n",
    "        return self.mu + self.sig*epsilon\n",
    "\n",
    "class GaussianMixturePrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Mixture of two Gaussian distributions as described in Bludell et al., 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_0: torch.Tensor, sigma_0: torch.Tensor, mu_1: torch.Tensor, sigma_1: torch.Tensor, pi: torch.Tensor):\n",
    "        super(GaussianMixturePrior, self).__init__()\n",
    "        self.mu_0 = mu_0 # mean of distribution 0\n",
    "        self.sigma_0 = sigma_0 # std of distrinution 0\n",
    "        self.mu_1 = mu_1 # mean of distribution 1\n",
    "        self.sigma_1 = sigma_1 # std of distribution 1\n",
    "        self.pi = pi # Probabilistic weight\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist_0 = Normal(loc=self.mu_0, scale=self.sigma_0)\n",
    "        dist_1 = Normal(loc=self.mu_1, scale=self.sigma_1)\n",
    "        ll_0 = dist_0.log_prob(values)\n",
    "        ll_1 = dist_1.log_prob(values)\n",
    "        return torch.log(self.pi * torch.exp(ll_0) + (1 - self.pi) * torch.exp(ll_1)).sum() \n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        # Creates a mixture of the two distributions depending on the size parameter pi\n",
    "        if np.random.rand() < self.pi:\n",
    "            return Normal(loc=self.mu_0, scale=self.sigma_0).sample()\n",
    "        else:\n",
    "            return Normal(loc=self.mu_1, scale=self.sigma_1).sample()\n",
    "        \n",
    "class BayesMultiLoss():\n",
    "    \n",
    "    \"\"\" Computes the KLD + NLL multi-objective loss. KLD is computed as mean of n-bathches. \n",
    "        The final loss is given as a mean over n-monte-carlo samples of the outputs returned by the forward pass of\n",
    "        BayesNet.\"\"\"\n",
    "    def __init__(self, net_outputs, targets, log_posterior, log_prior,\n",
    "                 batch_size, num_batches, method='exact'):\n",
    "        \n",
    "        # Define fields\n",
    "        self.net_outputs = net_outputs # Forward pass outputs\n",
    "        self.targets = targets         # y_batch targets\n",
    "        self.log_posterior=log_posterior # Log post\n",
    "        self.log_prior=log_prior     # Log prior\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.num_batches = num_batches # Number of batches\n",
    "        self.method = method # KLD method: exact or approx\n",
    "    \n",
    "    def __compute_kld_loss(self):\n",
    "        \"\"\" Computes the kld loss\"\"\"\n",
    "\n",
    "        if self.method == 'exact':\n",
    "            kld = self.log_posterior - self.log_prior\n",
    "            kld_scaled = kld / self.num_batches \n",
    "            return kld_scaled\n",
    "    \n",
    "        elif self.method =='approx':\n",
    "            log_ratio = self.log_prior - self.log_posterior\n",
    "            kld = (log_ratio.exp() -1) - log_ratio \n",
    "            kld_scaled = kld / self.num_batches\n",
    "        return kld_scaled\n",
    "            \n",
    "    def __compute_nll_loss(self):\n",
    "        \"\"\" Computes the NLL loss\"\"\"\n",
    "        loss = F.nll_loss(F.log_softmax(self.net_outputs, dim=1), self.targets, reduction='sum') \n",
    "        return loss\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        \"\"\" Computes the combined loss: KLD + NLL\"\"\"\n",
    "        kld = self.__compute_kld_loss()\n",
    "        nll = self.__compute_nll_loss()\n",
    "        multi_loss = kld + nll\n",
    "        return multi_loss\n",
    "    \n",
    "class BayesianLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a single Bayesian feedforward layer.\n",
    "    It maintains a prior and variational posterior for the weights (and biases)\n",
    "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Create a BayesianLayer.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param out_features: Number of output features\n",
    "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        # Set hyper priors: doubled because using guassian mixture distribution\n",
    "        mu_0_prior =  torch.tensor(0.0)\n",
    "        sigma_0_prior = torch.tensor(0.368)\n",
    "        mu_1_prior = torch.tensor(0.0)\n",
    "        sigma_1_prior = torch.tensor(0.00091)\n",
    "        pi_prior = torch.tensor(0.5)\n",
    "\n",
    "        # Define prior distribution\n",
    "        self.prior = GaussianMixturePrior(mu_0 = mu_0_prior,\n",
    "                                          sigma_0 = sigma_0_prior,\n",
    "                                          mu_1 = mu_1_prior,\n",
    "                                          sigma_1 = sigma_1_prior,\n",
    "                                          pi = pi_prior\n",
    "        )\n",
    "        assert isinstance(self.prior, ParameterDistribution)\n",
    "        assert not any(True for _ in self.prior.parameters()), 'Prior SHOULD NOT have parameters'\n",
    "\n",
    "        # Set intitial hyper poteriors\n",
    "        std_mu_init = torch.tensor(0.1) # Posterior distribution initial mean (multivariate diagonal guassian)\n",
    "        std_rho_init = torch.tensor(1.) # Posterio distribution Parameterisation of Std (multivariate diagonal guassian)\n",
    "        \n",
    "        # Initialize weights by sampling from normal distributions\n",
    "        # ... ( in_features = number of neurons in layer-1, out_features = number of connections going into layer)\n",
    "        w_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features, in_features))\n",
    "        w_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features, in_features))\n",
    "\n",
    "        # Convert sampled weights into torch parameters for optimisation\n",
    "        self.weights_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(w_mu_init),\n",
    "            rho = torch.nn.Parameter(w_rho_init)\n",
    "        )\n",
    "\n",
    "        # Error check\n",
    "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
    "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
    "\n",
    "        if self.use_bias:\n",
    "            # Initialize bias with zero mean and with parameterised std - samples n times where n=out_features\n",
    "            # ... each layer has out_features weights\n",
    "            b_mu_init = Normal(torch.tensor(0.), std_mu_init).sample((out_features,))\n",
    "            b_rho_init = Normal(torch.tensor(0.), std_rho_init).sample((out_features,))\n",
    "\n",
    "            # Use the same posterior family distribution and make them torch parameters for optimisation\n",
    "            self.bias_var_posterior = MultivariateDiagonalGaussian(\n",
    "            mu = torch.nn.Parameter(b_mu_init),\n",
    "            rho = torch.nn.Parameter(b_rho_init)\n",
    "        )\n",
    "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
    "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
    "        else:\n",
    "            self.bias_var_posterior = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Perform one forward pass through this layer.\n",
    "        \"\"\"\n",
    "        # Sample the weights (1st round is from initialised posterior)\n",
    "        weights = self.weights_var_posterior.sample()\n",
    "\n",
    "        # Generate the log-liklihood of the prior and log-posterior\n",
    "        log_prior = self.prior.log_likelihood(weights)\n",
    "        log_variational_posterior = self.weights_var_posterior.log_likelihood(weights)\n",
    "\n",
    "        # As in standard machine learning, we simply add on the bias term to each output in the next adjacent layer\n",
    "        if self.use_bias:\n",
    "            # Sample the bias posterios and get prior log-likelihood\n",
    "            bias = self.bias_var_posterior.sample()\n",
    "\n",
    "            log_prior += self.prior.log_likelihood(bias)\n",
    "            # Add on the terms to the variatinoal posterior\n",
    "            log_variational_posterior += self.bias_var_posterior.log_likelihood(bias)\n",
    "        else:\n",
    "            bias = None\n",
    "        # Compute the predictive outputs\n",
    "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior\n",
    "\n",
    "class BayesNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing a Bayesian feedforward neural network using BayesianLayer objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
    "        \"\"\"\n",
    "        Create a BNN.\n",
    "\n",
    "        :param in_features: Number of input features\n",
    "        :param hidden_features: Tuple where each entry corresponds to a (Bayesian) hidden layer with\n",
    "            the corresponding number of features.\n",
    "        :param out_features: Number of output features\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # Dynamically build the number of layers and their sizes\n",
    "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
    "        num_affine_maps = len(feature_sizes) - 1\n",
    "        self.layers = nn.ModuleList([\n",
    "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
    "            for idx in range(num_affine_maps)\n",
    "        ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Perform one forward pass through the BNN using a single set of weights\n",
    "        sampled from the variational posterior.\n",
    "\n",
    "        :param x: Input features, float tensor of shape (batch_size, in_features)\n",
    "        \"\"\"\n",
    "        # Initialize the \"summed\" log-prior likelihood and log-variational-posterior likelihood\n",
    "        log_prior = torch.tensor(0.0)\n",
    "        log_variational_posterior = torch.tensor(0.0)\n",
    "\n",
    "        for idx, current_layer in enumerate(self.layers):\n",
    "            x, log_prior_layer, log_variational_posterior_layer = current_layer(x)\n",
    "            if idx < len(self.layers) - 1:\n",
    "                x = self.activation(x)\n",
    "\n",
    "            log_prior += log_prior_layer\n",
    "            log_variational_posterior += log_variational_posterior_layer\n",
    "\n",
    "        return x, log_prior, log_variational_posterior\n",
    "\n",
    "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 100) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict class probabilities for the given features by sampling from this BNN.\n",
    "\n",
    "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
    "        :param num_mc_samples: Number of MC samples to take for prediction\n",
    "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
    "            such that the last dimension sums up to 1 for each row\n",
    "        \"\"\"\n",
    "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
    "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
    "\n",
    "        # assert estimated_probability.shape == (x.shape[0], 10)\n",
    "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0)) # Make sure the probabilities add up to 1\n",
    "        return estimated_probability, probability_samples\n",
    "    \n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "    BNN using Bayes by backprop\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Hyperparameters and general parameters\n",
    "        self.num_epochs = 1000  # number of training epochs\n",
    "        self.batch_size = 128  # training batch size\n",
    "        learning_rate = 1e-2  # training learning rates\n",
    "        hidden_layers = (100, 100)  # for each entry, creates a hidden layer with the corresponding number of units\n",
    "        self.print_interval = 100  # number of batches until updated metrics are displayed during training\n",
    "        self.n_mcs = 1 # Number of monte-carlo samples\n",
    "\n",
    "        # BayesNet\n",
    "        print('Using a BayesNet model')\n",
    "        self.network = BayesNet(in_features=13, hidden_features=hidden_layers, out_features=2)\n",
    "\n",
    "        # Optimizer for training\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def fit(self, dataset: torch.utils.data.Dataset):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        :param dataset: Dataset you should use for training\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
    "        )\n",
    "\n",
    "        # Place network into training mode\n",
    "        self.network.train()\n",
    "\n",
    "        progress_bar = trange(self.num_epochs)\n",
    "        for _ in progress_bar:\n",
    "            num_batches = len(train_loader)\n",
    "            for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
    "                # batch_x are of shape (batch_size, 13), batch_y are of shape (batch_size,)\n",
    "                # Convert the data-types\n",
    "                batch_x = torch.FloatTensor(batch_x)\n",
    "                self.network.zero_grad()\n",
    "            \n",
    "                # BayesNet training step via Bayes by backprop\n",
    "                assert isinstance(self.network, BayesNet)\n",
    "    \n",
    "                loss = torch.tensor([0.0])\n",
    "                for _ in range(self.n_mcs):\n",
    "                    current_logits = self.network(batch_x)\n",
    "                    outputs, log_prior, log_posterior = current_logits\n",
    "                    \n",
    "                    # Compute the losses \n",
    "                    batch_size = train_loader.batch_size\n",
    "                    BML = BayesMultiLoss(net_outputs=outputs, targets=batch_y, \n",
    "                                            log_posterior=log_posterior, log_prior=log_prior,\n",
    "                                            batch_size=batch_size, num_batches=num_batches, method='approx')\n",
    "                    \n",
    "                    loss += BML.compute_loss()\n",
    "                    \n",
    "                # Backpropagate to get the gradients\n",
    "                loss = loss/self.n_mcs\n",
    "                loss.backward()\n",
    "                    \n",
    "                \n",
    "                # Step the gradients\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # Update progress bar with accuracy occasionally\n",
    "                if batch_idx % self.print_interval == 0:    \n",
    "                    current_logits, _, _ = self.network(batch_x)\n",
    "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
    "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
    "                \n",
    "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Predict the class probabilities for the given data\n",
    "            :param data_loader: Data loader yielding the samples to predict on\n",
    "            :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
    "            \"\"\"\n",
    "\n",
    "            self.network.eval()\n",
    "\n",
    "            probability_batches = []\n",
    "            predictive_probability_dist_batches = []\n",
    "            for batch_x, _ in data_loader:\n",
    "                batch_x = torch.FloatTensor(batch_x)\n",
    "                current_probabilities, predictive_distribution = self.network.predict_probabilities(batch_x)\n",
    "                current_probabilities  = current_probabilities.detach().numpy()\n",
    "                predictive_distribution = predictive_distribution.detach().numpy()\n",
    "                print(current_probabilities.shape)\n",
    "                probability_batches.append(current_probabilities)\n",
    "                predictive_probability_dist_batches.append(predictive_distribution)\n",
    "\n",
    "            output = np.concatenate(probability_batches, axis=0)\n",
    "            output_pred_dist = predictive_probability_dist_batches\n",
    "            assert isinstance(output, np.ndarray)\n",
    "            # assert output.ndim == 2 and output.shape[1] == 10\n",
    "            assert np.allclose(np.sum(output, axis=1), 1.0)\n",
    "            return output, output_pred_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
