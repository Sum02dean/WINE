{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import typing\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange\n",
    "from torch.distributions import Normal\n",
    "from torch import distributions as dis\n",
    "\n",
    "class ParameterDistribution(torch.nn.Module, metaclass=abc.ABCMeta):\n",
    "    \"\"\"\n",
    "    Abstract class that models a distribution over model parameters,\n",
    "    usable for Bayes by backprop.\n",
    "    You can implement this class using any distribution you want\n",
    "    and try out different priors and variational posteriors.\n",
    "    All torch.nn.Parameter that you add in the __init__ method of this class\n",
    "    will automatically be registered and know to PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of the given values\n",
    "        :param values: Values to calculate the log-likelihood on\n",
    "        :return: Log-likelihood\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from this distribution.\n",
    "        Note that you only need to implement this method for variational posteriors, not priors.\n",
    "\n",
    "        :return: Sample from this distribution. The sample shape depends on your semantics.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        # DO NOT USE THIS METHOD\n",
    "        # We only implement it since torch.nn.Module requires a forward method\n",
    "        warnings.warn('ParameterDistribution should not be called! Use its explicit methods!')\n",
    "        return self.log_likelihood(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Implemented by Dean\n",
    "class UniveriateGaussianPrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Univeriate Guassian distribution \n",
    "    \"\"\"\n",
    "    def __init__(self, mu, sigma):\n",
    "        super(UniveriateGaussianPrior, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma # sigma is the standard deviation here, not the variance!\n",
    "    \n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist = Normal(loc=self.mu, scale=self.sigma)\n",
    "        log_likelihood = dist.log_prob(values).sum()\n",
    "        return log_likelihood\n",
    "\n",
    "    def sample(self):\n",
    "       return Normal(loc=self.mu, scale=self.sigma).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Multivariate diagonal Gaussian distribution,\n",
    "    i.e., assumes all elements to be independent Gaussians\n",
    "    but with different means and standard deviations.\n",
    "    This parameterizes the standard deviation via a parameter rho as\n",
    "    sigma = softplus(rho).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
    "        assert mu.size() == rho.size()\n",
    "        self.mu = mu\n",
    "        self.rho = rho\n",
    "        self.sig = (F.softplus(rho)*0.05 + 1e-5).detach()\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        likelihood = Normal(self.mu, self.sig).log_prob(values).sum()\n",
    "\n",
    "        return likelihood\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        epsilon = torch.distributions.Normal(0,1).sample(self.rho.size())\n",
    "        return self.mu + self.sig*epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixturePrior(ParameterDistribution):\n",
    "    \"\"\"\n",
    "    Mixture of two Gaussian distributions as described in Bludell et al., 2015.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu_0: torch.Tensor, sigma_0: torch.Tensor, mu_1: torch.Tensor, sigma_1: torch.Tensor, pi: torch.Tensor):\n",
    "        super(GaussianMixturePrior, self).__init__()  # always make sure to include the super-class init call!\n",
    "        self.mu_0 = mu_0 # mean of distribution 0\n",
    "        self.sigma_0 = sigma_0 # std of distrinution 0\n",
    "        self.mu_1 = mu_1 # mean of distribution 1\n",
    "        self.sigma_1 = sigma_1 # std of distribution 1\n",
    "        self.pi = pi # Probabilistic weight\n",
    "\n",
    "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
    "        dist_0 = Normal(loc=self.mu_0, scale=self.sigma_0)\n",
    "        dist_1 = Normal(loc=self.mu_1, scale=self.sigma_1)\n",
    "        ll_0 = dist_0.log_prob(values)\n",
    "        ll_1 = dist_1.log_prob(values)\n",
    "        return torch.log(self.pi * torch.exp(ll_0) + (1 - self.pi) * torch.exp(ll_1)).sum()\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        if np.random.rand() < self.pi:\n",
    "            return Normal(loc=self.mu_0, scale=self.sigma_0).sample()\n",
    "        else:\n",
    "            return Normal(loc=self.mu_1, scale=self.sigma_1).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
